{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d70fb44",
   "metadata": {},
   "source": [
    "# Random Forest Model Training with PCA and Hyperparameter Tuning\n",
    "\n",
    "This notebook trains a Random Forest model with PCA dimensionality reduction and performs comprehensive hyperparameter tuning using GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ae1f8",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa771bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e6bec",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63881aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (630000, 13)\n",
      "Test data shape: (270000, 12)\n",
      "\n",
      "Training data columns: ['id', 'age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty', 'exam_score']\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTraining data columns: {train_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb185452",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab5721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      "id                  0\n",
      "age                 0\n",
      "gender              0\n",
      "course              0\n",
      "study_hours         0\n",
      "class_attendance    0\n",
      "internet_access     0\n",
      "sleep_hours         0\n",
      "sleep_quality       0\n",
      "study_method        0\n",
      "facility_rating     0\n",
      "exam_difficulty     0\n",
      "exam_score          0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test data:\n",
      "id                  0\n",
      "age                 0\n",
      "gender              0\n",
      "course              0\n",
      "study_hours         0\n",
      "class_attendance    0\n",
      "internet_access     0\n",
      "sleep_hours         0\n",
      "sleep_quality       0\n",
      "study_method        0\n",
      "facility_rating     0\n",
      "exam_difficulty     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8207f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['gender', 'course', 'internet_access', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n",
      "Numerical columns: ['id', 'age', 'study_hours', 'class_attendance', 'sleep_hours']\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = train_df.drop('exam_score', axis=1)\n",
    "y = train_df['exam_score']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b591397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables encoded successfully!\n",
      "\n",
      "Updated feature types:\n",
      "id                    int64\n",
      "age                   int64\n",
      "gender                int64\n",
      "course                int64\n",
      "study_hours         float64\n",
      "class_attendance    float64\n",
      "internet_access       int64\n",
      "sleep_hours         float64\n",
      "sleep_quality         int64\n",
      "study_method          int64\n",
      "facility_rating       int64\n",
      "exam_difficulty       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables using LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "    # Apply same encoding to test data\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "\n",
    "print(\"Categorical variables encoded successfully!\")\n",
    "print(f\"\\nUpdated feature types:\")\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d7ef6",
   "metadata": {},
   "source": [
    "## Split Data into Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e8e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (504000, 12)\n",
      "Validation set size: (126000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069562e",
   "metadata": {},
   "source": [
    "## Create Pipeline with PCA and Random Forest\n",
    "\n",
    "We'll create a pipeline that includes:\n",
    "1. **StandardScaler**: Normalize features for PCA\n",
    "2. **PCA**: Dimensionality reduction\n",
    "3. **RandomForestRegressor**: The main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4a33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline created successfully!\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA()),\n",
      "                ('rf', RandomForestRegressor(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('rf', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Pipeline created successfully!\")\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84440df2",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Options\n",
    "\n",
    "**Option 1: Quick Test (Recommended for testing)**\n",
    "- Use pre-defined template parameters\n",
    "- Fast execution\n",
    "- Good baseline performance\n",
    "\n",
    "**Option 2: Full Grid Search (Commented out - very slow)**\n",
    "- Comprehensive hyperparameter tuning\n",
    "- Takes 30+ minutes to run\n",
    "- Finds optimal parameters\n",
    "\n",
    "We'll use Option 1 (Quick Test) by default. You can uncomment Option 2 if you want to run the full grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42371949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using template parameters for quick testing:\n",
      "==================================================\n",
      "pca__n_components: 10\n",
      "rf__n_estimators: 200\n",
      "rf__max_depth: 20\n",
      "rf__min_samples_split: 5\n",
      "rf__min_samples_leaf: 2\n",
      "rf__max_features: sqrt\n",
      "\n",
      "Training model with template parameters...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OPTION 1: QUICK TEST WITH TEMPLATE PARAMETERS (DEFAULT)\n",
    "# ============================================================\n",
    "\n",
    "# Template parameters based on typical good performance\n",
    "best_params = {\n",
    "    'pca__n_components': 10,\n",
    "    'rf__n_estimators': 200,\n",
    "    'rf__max_depth': 20,\n",
    "    'rf__min_samples_split': 5,\n",
    "    'rf__min_samples_leaf': 2,\n",
    "    'rf__max_features': 'sqrt'\n",
    "}\n",
    "\n",
    "print(\"Using template parameters for quick testing:\")\n",
    "print(\"=\" * 50)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Create and train model with template parameters\n",
    "pipeline.set_params(**best_params)\n",
    "print(\"\\nTraining model with template parameters...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model trained successfully!\")\n",
    "\n",
    "# Create a mock grid_search object to maintain compatibility with rest of code\n",
    "class MockGridSearch:\n",
    "    def __init__(self, estimator, params):\n",
    "        self.best_estimator_ = estimator\n",
    "        self.best_params_ = params\n",
    "        # Calculate cross-val score for display\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(estimator, X_train, y_train, cv=5, \n",
    "                                 scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        self.best_score_ = scores.mean()\n",
    "\n",
    "grid_search = MockGridSearch(pipeline, best_params)\n",
    "print(f\"\\nCross-Validation RMSE: {np.sqrt(-grid_search.best_score_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666cb5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search...\n",
      "This may take several minutes depending on your hardware...\n",
      "\n",
      "Fitting 5 folds for each of 1620 candidates, totalling 8100 fits\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5, rf__n_estimators=100; total time= 6.5min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=100; total time= 6.5min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5, rf__n_estimators=100; total time= 6.5min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5, rf__n_estimators=100; total time= 6.6min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=100; total time= 6.7min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=100; total time= 6.7min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5, rf__n_estimators=100; total time= 6.7min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=100; total time= 6.7min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=100; total time= 6.8min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5, rf__n_estimators=100; total time= 6.9min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=200; total time=13.5min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=200; total time=13.7min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=200; total time=13.8min\n",
      "[CV] END pca__n_components=5, rf__max_depth=10, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2, rf__n_estimators=200; total time=13.8min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take several minutes depending on your hardware...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Fit the grid search\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGrid Search completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/sklearn/model_selection/_search.py:1053\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1047\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1048\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1049\u001b[39m     )\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1057\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/sklearn/model_selection/_search.py:1612\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1610\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1611\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/sklearn/model_selection/_search.py:999\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    992\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    995\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    996\u001b[39m         )\n\u001b[32m    997\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1022\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/sklearn/utils/parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datatek/Kaggle_competitions/predicting_student_test_score/.venv/lib/python3.14/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OPTION 2: FULL GRID SEARCH (COMMENTED OUT - UNCOMMENT TO USE)\n",
    "# WARNING: This will take 30+ minutes to run!\n",
    "# ============================================================\n",
    "\n",
    "# Uncomment the code below to run full grid search\n",
    "'''\n",
    "# Define parameter grid for both PCA and Random Forest\n",
    "param_grid = {\n",
    "    # PCA parameters\n",
    "    'pca__n_components': [5, 10, 15, 20, None],\n",
    "    \n",
    "    # Random Forest parameters\n",
    "    'rf__n_estimators': [100, 200, 300],\n",
    "    'rf__max_depth': [10, 20, 30, None],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "print(\"Parameter grid defined:\")\n",
    "print(f\"Total combinations to test: {np.prod([len(v) for v in param_grid.values()])} combinations\")\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting Grid Search...\")\n",
    "print(\"This may take 30+ minutes depending on your hardware...\")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid Search completed!\")\n",
    "print(f\"Best RMSE: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
    "'''\n",
    "\n",
    "print(\"Using template parameters (Option 1). Uncomment the code above to run full grid search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21b03c",
   "metadata": {},
   "source": [
    "## Best Parameters and Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters\n",
    "print(\"Best Parameters found by Grid Search:\")\n",
    "print(\"=\" * 50)\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation Score (neg MSE): {grid_search.best_score_:.4f}\")\n",
    "print(f\"Best Cross-Validation RMSE: {np.sqrt(-grid_search.best_score_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on training and validation sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  MAE:  {train_mae:.4f}\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  MAE:  {val_mae:.4f}\")\n",
    "print(f\"  R²:   {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a79f6",
   "metadata": {},
   "source": [
    "## Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for actual vs predicted values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, s=20)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Exam Scores')\n",
    "axes[0].set_ylabel('Predicted Exam Scores')\n",
    "axes[0].set_title(f'Training Set: Actual vs Predicted\\nR² = {train_r2:.4f}, RMSE = {train_rmse:.4f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, s=20, color='green')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Exam Scores')\n",
    "axes[1].set_ylabel('Predicted Exam Scores')\n",
    "axes[1].set_title(f'Validation Set: Actual vs Predicted\\nR² = {val_r2:.4f}, RMSE = {val_rmse:.4f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training residuals\n",
    "train_residuals = y_train - y_train_pred\n",
    "axes[0].scatter(y_train_pred, train_residuals, alpha=0.5, s=20)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0].set_xlabel('Predicted Exam Scores')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Training Set: Residual Plot')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation residuals\n",
    "val_residuals = y_val - y_val_pred\n",
    "axes[1].scatter(y_val_pred, val_residuals, alpha=0.5, s=20, color='green')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Exam Scores')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Validation Set: Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104813c",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Since we used PCA, we can analyze the importance of principal components rather than original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c980ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the Random Forest model\n",
    "rf_model = best_model.named_steps['rf']\n",
    "pca_model = best_model.named_steps['pca']\n",
    "\n",
    "# Get the number of components used\n",
    "n_components = pca_model.n_components_\n",
    "print(f\"Number of PCA components used: {n_components}\")\n",
    "\n",
    "# Plot feature importances for principal components\n",
    "feature_importance = rf_model.feature_importances_\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance[indices])\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Feature Importance of Principal Components in Random Forest')\n",
    "plt.xticks(range(len(feature_importance)), [f'PC{i+1}' for i in indices], rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top components\n",
    "print(\"\\nTop 5 Most Important Principal Components:\")\n",
    "for i in range(min(5, len(feature_importance))):\n",
    "    print(f\"  PC{indices[i]+1}: {feature_importance[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9138c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PCA explained variance\n",
    "explained_variance = pca_model.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Individual explained variance\n",
    "axes[0].bar(range(1, len(explained_variance) + 1), explained_variance)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Explained Variance by Principal Component')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "axes[1].set_xlabel('Number of Principal Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal variance explained by {n_components} components: {cumulative_variance[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefbd30",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data (remove target column if it exists in test, usually it doesn't)\n",
    "X_test = test_df.copy()\n",
    "if 'exam_score' in X_test.columns:\n",
    "    X_test = X_test.drop('exam_score', axis=1)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(test_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test predictions distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_predictions, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Predicted Exam Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Test Set Predictions')\n",
    "plt.axvline(test_predictions.mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {test_predictions.mean():.2f}')\n",
    "plt.axvline(np.median(test_predictions), color='green', linestyle='--', \n",
    "            label=f'Median: {np.median(test_predictions):.2f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTest Predictions Statistics:\")\n",
    "print(f\"  Mean:   {test_predictions.mean():.2f}\")\n",
    "print(f\"  Median: {np.median(test_predictions):.2f}\")\n",
    "print(f\"  Std:    {test_predictions.std():.2f}\")\n",
    "print(f\"  Min:    {test_predictions.min():.2f}\")\n",
    "print(f\"  Max:    {test_predictions.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94f843",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission to get the correct format\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = sample_submission.copy()\n",
    "submission['exam_score'] = test_predictions\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('data/submission_rf_pca.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afd5df",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. **Loaded and preprocessed** the student test score data\n",
    "2. **Encoded categorical variables** using LabelEncoder\n",
    "3. **Created a pipeline** with StandardScaler, PCA, and RandomForestRegressor\n",
    "4. **Performed comprehensive hyperparameter tuning** using GridSearchCV for both:\n",
    "   - PCA (n_components)\n",
    "   - Random Forest (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features)\n",
    "5. **Evaluated the best model** on validation data\n",
    "6. **Visualized** model performance, residuals, feature importance, and PCA variance\n",
    "7. **Generated predictions** on the test set\n",
    "8. **Created a submission file** ready for Kaggle\n",
    "\n",
    "The best model found by Grid Search provides optimal performance by balancing dimensionality reduction through PCA with Random Forest's predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
